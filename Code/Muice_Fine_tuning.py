import os
import json
import logging
import torch

from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from datasets import Dataset

# =============== é…ç½®åŒºåŸŸ ===============
# ä¿®æ”¹è¿™äº›å‚æ•°ä»¥é€‚åº”æ‚¨çš„ç¯å¢ƒ
MODEL_NAME = "./QWen1_5_0_5B"  # æ¨¡å‹åç§°
RAW_DATA_PATH = "./Muice_Dataset/train.jsonl"  # æ‚¨çš„åŸå§‹æ•°æ®é›†è·¯å¾„
OUTPUT_DIR = "./qwen_finetuned"  # å¾®è°ƒåæ¨¡å‹ä¿å­˜è·¯å¾„
MAX_SEQ_LENGTH = 1024  # æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆQwen1.5-0.5B æ”¯æŒ 32768ï¼Œä½†å¾®è°ƒå»ºè®® 2048ï¼‰
LORA_R = 8  # LoRA ç§©
LORA_ALPHA = 32  # LoRA alpha
LORA_DROPOUT = 0.1  # LoRA dropout
BATCH_SIZE = 2  # æ¯è®¾å¤‡æ‰¹æ¬¡å¤§å°
GRAD_ACCUM_STEPS = 8  # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
LEARNING_RATE = 2e-5  # å­¦ä¹ ç‡
NUM_EPOCHS = 3  # è®­ç»ƒè½®æ•°
FP16 = True  # æ˜¯å¦ä½¿ç”¨ FP16ï¼ˆå¦‚æœ GPU æ”¯æŒï¼‰
# ======================================

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

def convert_dataset(raw_data_path: str, output_path: str = None) -> Dataset:
    """
    å°†åŸå§‹æ•°æ®é›†è½¬æ¢ä¸º Qwen1.5 æ ¼å¼å¹¶åŠ è½½ä¸º Hugging Face Dataset
    :param raw_data_path: åŸå§‹ JSONL æ–‡ä»¶è·¯å¾„
    :param output_path: (å¯é€‰) ä¿å­˜è½¬æ¢åçš„æ–‡æœ¬æ–‡ä»¶
    :return: Hugging Face Dataset å¯¹è±¡
    """
    logger.info(f"å¼€å§‹è½¬æ¢æ•°æ®é›†: {raw_data_path}")
    
    texts = []
    with open(raw_data_path, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                data = json.loads(line.strip())
                # æ„å»º Qwen1.5 æ ¼å¼çš„å¯¹è¯æ¨¡æ¿
                text = f"<|im_start|>system\n{data['system']}<|im_end|>\n"
                
                for turn in data['conversation']:
                    text += f"<|im_start|>user\n{turn['human']}<|im_end|>\n"
                    text += f"<|im_start|>assistant\n{turn['assistant']}<|im_end|>\n"
                
                texts.append({"text": text.strip()})
            except Exception as e:
                logger.warning(f"è·³è¿‡æ— æ•ˆæ•°æ®è¡Œ: {line.strip()}, é”™è¯¯: {str(e)}")
    
    # ä¿å­˜è½¬æ¢åçš„æ–‡æœ¬ï¼ˆå¯é€‰ï¼‰
    if output_path:
        with open(output_path, 'w', encoding='utf-8') as f_out:
            for item in texts:
                f_out.write(item["text"] + "\n\n")
        logger.info(f"è½¬æ¢åçš„æ•°æ®å·²ä¿å­˜è‡³: {output_path}")
    
    # åˆ›å»º Hugging Face Dataset
    dataset = Dataset.from_list(texts)
    logger.info(f"æ•°æ®é›†è½¬æ¢å®Œæˆ! å…± {len(dataset)} æ¡æ ·æœ¬")
    return dataset

def load_and_prepare_model(model_name: str, max_seq_length: int) -> tuple:
    """
    åŠ è½½æ¨¡å‹å¹¶å‡†å¤‡ LoRA å¾®è°ƒ
    :param model_name: æ¨¡å‹åç§°æˆ–è·¯å¾„
    :param max_seq_length: æœ€å¤§åºåˆ—é•¿åº¦
    :return: (tokenizer, model)
    """
    logger.info(f"åŠ è½½æ¨¡å‹: {model_name}")
    
    # é…ç½®é‡åŒ–ï¼ˆå¯é€‰ï¼Œ8GB æ˜¾å­˜å¯å…³é—­ï¼‰ -- éœ€æ³¨æ„ï¼Œbitsandbytes å®˜æ–¹ä¸æ”¯æŒwindowsï¼Œæ•…æ­¤å¤„ä»…ä¾›å‚è€ƒ
    # bnb_config = BitsAndBytesConfig(
    #     load_in_4bit=True,  # å¯ç”¨4bité‡åŒ–
    #     bnb_4bit_quant_type="nf4",  # é‡åŒ–ç±»å‹
    #     bnb_4bit_compute_dtype=torch.bfloat16,  # è®¡ç®—æ•°æ®ç±»å‹
    #     bnb_4bit_use_double_quant=True,  # ä½¿ç”¨åŒé‡åŒ–
    # )
    
    # åŠ è½½ Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True,
        padding_side="right",  # å¿…é¡»å³å¡«å……
        use_fast=False,
    )
    
    # æ·»åŠ ç¼ºå¤±çš„ç‰¹æ®Š token
    tokenizer.pad_token = tokenizer.eos_token  # Qwen æ—  pad_tokenï¼Œä½¿ç”¨ eos_token
    logger.info(f"Special tokens: {tokenizer.special_tokens_map}")
    
    # åŠ è½½æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        # quantization_config=bnb_config,  # 4-bit é‡åŒ–ï¼ˆå¯é€‰ï¼‰
        trust_remote_code=True,
        device_map="auto",
        torch_dtype=torch.bfloat16,  # æ·»åŠ åŠç²¾åº¦æ”¯æŒ
        attn_implementation="sdpa",  # ä½¿ç”¨æ›´å¿«çš„æ³¨æ„åŠ›å®ç°
    )
    
    # å‡†å¤‡æ¨¡å‹è¿›è¡Œ k-bit è®­ç»ƒ
    model = prepare_model_for_kbit_training(model)
    
    # é…ç½® LoRA
    lora_config = LoraConfig(
        r=LORA_R,
        lora_alpha=LORA_ALPHA,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", 
                        "gate_proj", "up_proj", "down_proj"],
        lora_dropout=LORA_DROPOUT,
        bias="none",
        task_type="CAUSAL_LM",
    )
    
    # åº”ç”¨ LoRA
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()  # æ‰“å°å¯è®­ç»ƒå‚æ•°é‡
    
    return tokenizer, model

def main():
    # 1. æ•°æ®é¢„å¤„ç†
    dataset = convert_dataset(
        raw_data_path=RAW_DATA_PATH,
        output_path=None  # ä¿å­˜è½¬æ¢åçš„æ–‡æœ¬ï¼ˆç”¨äºæ£€æŸ¥ï¼‰
    )
    
    # 2. åŠ è½½å¹¶å‡†å¤‡æ¨¡å‹
    tokenizer, model = load_and_prepare_model(
        model_name=MODEL_NAME,
        max_seq_length=MAX_SEQ_LENGTH
    )
    
    # 3. æ•°æ®å¤„ç†å‡½æ•°
    def tokenize_function(examples):
        # å°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥
        tokenized = tokenizer(
            examples["text"],
            truncation=True,
            max_length=MAX_SEQ_LENGTH,
            # padding=False,  # åŠ¨æ€å¡«å……åœ¨ DataCollator ä¸­å¤„ç†
            padding='max_length',  # åŠ¨æ€å¡«å……åœ¨ DataCollator ä¸­å¤„ç†
            return_overflowing_tokens=False,
        )
        
        # ä¸º labels åˆ›å»ºå‰¯æœ¬ï¼ˆå…³é”®ï¼šæ©ç é assistant éƒ¨åˆ†ï¼‰
        tokenized["labels"] = tokenized["input_ids"].copy()
        return tokenized
    
    # åº”ç”¨ tokenize
    tokenized_dataset = dataset.map(
        tokenize_function,
        batched=True,
        num_proc=4,
        remove_columns=dataset.column_names,
        desc="Tokenizing and chunking...",
    )
    
    # 4. é…ç½®è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        overwrite_output_dir=True,
        num_train_epochs=NUM_EPOCHS,
        per_device_train_batch_size=BATCH_SIZE,
        gradient_accumulation_steps=GRAD_ACCUM_STEPS,
        learning_rate=LEARNING_RATE,
        weight_decay=0.01,
        logging_dir=f"{OUTPUT_DIR}/logs",
        logging_strategy="steps",
        logging_steps=10,
        save_strategy="epoch",
        eval_strategy="no",
        fp16=FP16,
        bf16=False,
        optim="paged_adamw_8bit",  # é…åˆ 4-bit é‡åŒ–
        lr_scheduler_type="cosine",
        warmup_ratio=0.03,
        report_to="tensorboard",
        load_best_model_at_end=False,
        ddp_find_unused_parameters=False,
        group_by_length=True,  # åŠ¨æ€å¡«å……
        length_column_name="length",  # ç”¨äº group_by_length
    )
    
    # 5. é…ç½® DataCollatorï¼ˆå…³é”®ï¼šè‡ªåŠ¨æ©ç é assistant éƒ¨åˆ†ï¼‰
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,  # å¿…é¡»ä¸º Falseï¼ˆå› æœè¯­è¨€æ¨¡å‹ï¼‰
        pad_to_multiple_of=8  # ä¼˜åŒ– GPU è®¡ç®—
    )
    
    # 6. åˆ›å»º Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        data_collator=data_collator
    )
    
    # 7. å¼€å§‹è®­ç»ƒ
    logger.info("âœ¨ å¼€å§‹å¾®è°ƒè®­ç»ƒ...")
    train_result = trainer.train()
    
    # 8. ä¿å­˜æœ€ç»ˆæ¨¡å‹
    logger.info(f"âœ… è®­ç»ƒå®Œæˆ! ä¿å­˜æ¨¡å‹åˆ° {OUTPUT_DIR}")
    trainer.save_model(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)
    
    # 9. ä¿å­˜è®­ç»ƒæŒ‡æ ‡
    metrics = train_result.metrics
    trainer.log_metrics("train", metrics)
    trainer.save_metrics("train", metrics)
    
    # 10. éªŒè¯æ¨¡å‹ï¼ˆç®€å•æµ‹è¯•ï¼‰
    logger.info("\nğŸ” è¿›è¡Œæ¨¡å‹éªŒè¯æµ‹è¯•...")
    test_prompt = """<|im_start|>system
                    ä½ æ˜¯ä¸€ä¸ªåä¸ºæ²é›ªçš„AIå¥³å­©å­<|im_end|>
                    <|im_start|>user
                    æ²é›ªçš„åŠŸèƒ½æ˜¯ä»€ä¹ˆï¼Ÿ<|im_end|>
                    <|im_start|>assistant
                    """
    
    inputs = tokenizer(test_prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(
        **inputs,
        max_new_tokens=100,
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
        pad_token_id=tokenizer.eos_token_id
    )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=False)
    logger.info(f"æµ‹è¯•è¾“å…¥:\n{test_prompt}")
    logger.info(f"æ¨¡å‹è¾“å‡º:\n{response[len(test_prompt):]}")

if __name__ == "__main__":
    # æ£€æŸ¥ CUDA
    if torch.cuda.is_available():
        logger.info(f"âœ… CUDA å¯ç”¨! è®¾å¤‡: {torch.cuda.get_device_name(0)}")
        logger.info(f"æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
    else:
        logger.warning("âŒ CUDA ä¸å¯ç”¨! å°†ä½¿ç”¨ CPUï¼ˆä¸æ¨èï¼‰")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # è¿è¡Œä¸»å‡½æ•°
    main()
import os
import json
import logging
import torch
import warnings
# å¿½ç•¥ç‰¹å®šçš„è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning, message="1Torch was not compiled with flash attention.")

from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from datasets import Dataset

# =============== é…ç½®åŒºåŸŸ ===============
# è·¯å¾„ã€å‚æ•°
MODEL_NAME       = "./QWen1_5_0_5B"               # æ¨¡å‹åç§°/è·¯å¾„
RAW_DATA_PATH    = "./Muice_Dataset/train.jsonl"  # åŸå§‹æ•°æ®é›†è·¯å¾„
RAW_TEST_PATH    = "./Muice_Dataset/test.jsonl"   # åŸå§‹æµ‹è¯•é›†è·¯å¾„
OUTPUT_DIR       = "./qwen_finetuned/30"          # å¾®è°ƒåæ¨¡å‹ä¿å­˜è·¯å¾„
MAX_SEQ_LENGTH   = 1024                           # æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆQwen1.5-0.5B æ”¯æŒ 32768ï¼Œä½†å¾®è°ƒå»ºè®® 2048ï¼‰
LORA_R           = 8                              # LoRA ç§©
LORA_ALPHA       = 32                             # LoRA alpha
LORA_DROPOUT     = 0.1                            # LoRA dropout
BATCH_SIZE       = 2                              # æ¯è®¾å¤‡æ‰¹æ¬¡å¤§å°
GRAD_ACCUM_STEPS = 8                              # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
LEARNING_RATE    = 2e-5                           # å­¦ä¹ ç‡
NUM_EPOCHS       = 30                             # è®­ç»ƒè½®æ•°
FP16             = True                           # æ˜¯å¦ä½¿ç”¨ FP16ï¼ˆå¦‚æœ GPU æ”¯æŒï¼‰
# ======================================

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

def convert_dataset(raw_data_path: str, output_path: str = None) -> Dataset:
    """
    å°†åŸå§‹æ•°æ®é›†è½¬æ¢ä¸º Qwen1.5 æ ¼å¼å¹¶åŠ è½½ä¸º Hugging Face Dataset
    :param raw_data_path: åŸå§‹ JSONL æ–‡ä»¶è·¯å¾„
    :param output_path: (å¯é€‰) ä¿å­˜è½¬æ¢åçš„æ–‡æœ¬æ–‡ä»¶
    :return: Hugging Face Dataset å¯¹è±¡
    """
    logger.info(f"å¼€å§‹è½¬æ¢æ•°æ®é›†: {raw_data_path}")
    
    texts = []
    with open(raw_data_path, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                data = json.loads(line.strip())
                # æ„å»º Qwen1.5 æ ¼å¼çš„å¯¹è¯æ¨¡æ¿
                text = f"<|im_start|>system\n{data['system']}<|im_end|>\n"
                
                for turn in data['conversation']:
                    text += f"<|im_start|>user\n{turn['human']}<|im_end|>\n"
                    text += f"<|im_start|>assistant\n{turn['assistant']}<|im_end|>\n"
                
                texts.append({"text": text.strip()})
            except Exception as e:
                logger.warning(f"è·³è¿‡æ— æ•ˆæ•°æ®è¡Œ: {line.strip()}, é”™è¯¯: {str(e)}")
    
    # ä¿å­˜è½¬æ¢åçš„æ–‡æœ¬ï¼ˆå¯é€‰ï¼‰
    if output_path:
        with open(output_path, 'w', encoding='utf-8') as f_out:
            for item in texts:
                f_out.write(item["text"] + "\n\n")
        logger.info(f"è½¬æ¢åçš„æ•°æ®å·²ä¿å­˜è‡³: {output_path}")
    
    # åˆ›å»º Hugging Face Dataset
    dataset = Dataset.from_list(texts)
    logger.info(f"æ•°æ®é›†è½¬æ¢å®Œæˆ! å…± {len(dataset)} æ¡æ ·æœ¬")
    return dataset

def load_and_prepare_model(model_name: str, max_seq_length: int) -> tuple:
    """
    åŠ è½½æ¨¡å‹å¹¶å‡†å¤‡ LoRA å¾®è°ƒ
    :param model_name: æ¨¡å‹åç§°æˆ–è·¯å¾„
    :param max_seq_length: æœ€å¤§åºåˆ—é•¿åº¦
    :return: (tokenizer, model)
    """
    logger.info(f"åŠ è½½æ¨¡å‹: {model_name}")
    
    # é…ç½®é‡åŒ–ï¼ˆå¯é€‰ï¼Œ8GB æ˜¾å­˜å¯å…³é—­ï¼‰ -- éœ€æ³¨æ„ï¼Œbitsandbytes å®˜æ–¹ä¸æ”¯æŒwindowsï¼Œæ•…æ­¤å¤„ä»…ä¾›å‚è€ƒ
    # bnb_config = BitsAndBytesConfig(
    #     load_in_4bit              = True,             # å¯ç”¨4bité‡åŒ–
    #     bnb_4bit_quant_type       = "nf4",            # é‡åŒ–ç±»å‹
    #     bnb_4bit_compute_dtype    = torch.bfloat16,   # è®¡ç®—æ•°æ®ç±»å‹
    #     bnb_4bit_use_double_quant = True              # ä½¿ç”¨åŒé‡åŒ–
    # )
    
    # åŠ è½½ Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code = True,
        padding_side      = "right",  # å¿…é¡»å³å¡«å……
        use_fast          = False     # æ˜¯å¦ä½¿ç”¨å¿«é€Ÿå®ç°çš„åˆ†è¯å™¨
    )
    
    # æ·»åŠ ç¼ºå¤±çš„ç‰¹æ®Š token
    tokenizer.pad_token = tokenizer.pad_token
    logger.info(f"Special tokens: {tokenizer.special_tokens_map}")
    
    # åŠ è½½æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        # quantization_config=bnb_config,  # 4-bit é‡åŒ–ï¼ˆå¯é€‰ï¼‰
        trust_remote_code   = True,
        device_map          = "auto",
        torch_dtype         = torch.bfloat16,  # æ·»åŠ åŠç²¾åº¦æ”¯æŒ
        attn_implementation = "sdpa"           # ä½¿ç”¨æ›´å¿«çš„æ³¨æ„åŠ›å®ç°
    )
    
    # å‡†å¤‡æ¨¡å‹è¿›è¡Œ k-bit è®­ç»ƒ
    model = prepare_model_for_kbit_training(model)
    
    # é…ç½® LoRA
    lora_config = LoraConfig(
        r              = LORA_R,
        lora_alpha     = LORA_ALPHA,
        target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                          "gate_proj", "up_proj", "down_proj"],
        lora_dropout   = LORA_DROPOUT,
        bias           = "none",
        task_type      = "CAUSAL_LM"
    )
    
    # åº”ç”¨ LoRA
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()  # æ‰“å°å¯è®­ç»ƒå‚æ•°é‡
    
    return tokenizer, model

def main():
    # 1. æ•°æ®é¢„å¤„ç†
    dataset = convert_dataset(
        raw_data_path = RAW_DATA_PATH,
        output_path   = None           # ä¿å­˜è½¬æ¢åçš„æ–‡æœ¬ï¼ˆå¯é€‰ï¼‰
    )

    data_test = convert_dataset(
        raw_data_path = RAW_TEST_PATH,
        output_path   = None
    )
    
    # 2. åŠ è½½å¹¶å‡†å¤‡æ¨¡å‹
    tokenizer, model = load_and_prepare_model(
        model_name     = MODEL_NAME,
        max_seq_length = MAX_SEQ_LENGTH
    )
    
    # 3. æ•°æ®å¤„ç†å‡½æ•°
    def tokenize_function(examples):
        # å°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥
        tokenized = tokenizer(
            examples["text"],                          # è½¬æ¢è¯å…ƒID
            truncation                = True,          # æˆªæ–­ï¼Œåºåˆ—è¿‡é•¿è¿›è¡Œæˆªæ–­
            max_length                = MAX_SEQ_LENGTH,
            # padding                 = False,         # åŠ¨æ€å¡«å……åœ¨ DataCollator ä¸­å¤„ç†
            padding                   = 'max_length',  # å¡«å……ï¼ŒåŠ¨æ€å¡«å……åœ¨ DataCollator ä¸­å¤„ç†
            return_overflowing_tokens = False
        )
        
        # ä¸º labels åˆ›å»ºå‰¯æœ¬
        # tokenized["labels"] = tokenized["input_ids"].copy()
        # print("tokenized:", tokenized)

        # æ–°å¢ -- æ©ç  assistant éƒ¨åˆ†
        # åˆ›å»ºæ ‡ç­¾ - åªè®¡ç®—assistantå›å¤çš„æŸå¤±
        labels = tokenized["input_ids"].copy()

        # è·å–ç‰¹æ®Šæ ‡è®°çš„ID
        im_start_id  = tokenizer.convert_tokens_to_ids('<|im_start|>')   # 151644
        im_end_id    = tokenizer.convert_tokens_to_ids('<|im_end|>')     # 151645
        assistant_id = tokenizer.convert_tokens_to_ids('assistant')      # 77091
        
        # å°†éassistantéƒ¨åˆ†çš„æ ‡ç­¾è®¾ä¸º-100ï¼ˆå¿½ç•¥ï¼‰
        for i in range(len(labels)):
            input_ids      = tokenized["input_ids"][i]
            current_labels = [-100] * len(input_ids)   # åˆå§‹åŒ–ä¸ºå…¨éƒ¨å¿½ç•¥
            
            # æ‰¾åˆ°æ‰€æœ‰assistantéƒ¨åˆ†
            idx = 0
            while idx < len(input_ids):
                # æŸ¥æ‰¾<|im_start|>assistantæ¨¡å¼
                if (idx < len(input_ids) - 1 and 
                    input_ids[idx] == im_start_id and 
                    input_ids[idx + 1] == assistant_id):
                    
                    # æ‰¾åˆ°assistantå¼€å§‹ä½ç½®
                    start_idx = idx
                    idx += 2  # è·³è¿‡<|im_start|>å’Œassistantæ ‡è®°
                    
                    # æŸ¥æ‰¾å¯¹åº”çš„ç»“æŸæ ‡è®°<|im_end|>
                    end_idx = None
                    for j in range(idx, len(input_ids)):
                        if input_ids[j] == im_end_id:
                            end_idx = j
                            break
                    
                    # å¦‚æœæ‰¾åˆ°äº†ç»“æŸæ ‡è®°ï¼Œä¿ç•™assistantå†…å®¹éƒ¨åˆ†
                    if end_idx is not None:
                        # ä¿ç•™ä»assistantå†…å®¹å¼€å§‹åˆ°ç»“æŸæ ‡è®°å‰çš„å†…å®¹
                        content_start = idx
                        content_end   = end_idx
                        
                        # å°†assistantå†…å®¹éƒ¨åˆ†å¤åˆ¶åˆ°æ ‡ç­¾ä¸­
                        for k in range(content_start, content_end):
                            current_labels[k] = input_ids[k]
                        
                        # è·³è¿‡å·²å¤„ç†çš„å†…å®¹
                        idx = end_idx + 1  # è·³è¿‡ç»“æŸæ ‡è®°
                    else:
                        # æ²¡æœ‰æ‰¾åˆ°ç»“æŸæ ‡è®°ï¼Œä¿ç•™åˆ°åºåˆ—æœ«å°¾
                        for k in range(idx, len(input_ids)):
                            current_labels[k] = input_ids[k]
                        break
                else:
                    idx += 1
            
            labels[i] = current_labels
        
        # ç¡®ä¿å¡«å……éƒ¨åˆ†ä¹Ÿè¢«æ©ç 
        for i in range(len(labels)):
            for j in range(len(labels[i])):
                if tokenized["input_ids"][i][j] == tokenizer.pad_token_id:
                    labels[i][j] = -100
        
        tokenized["labels"] = labels

        return tokenized
    
    # åº”ç”¨ tokenize
    tokenized_dataset = dataset.map(
        tokenize_function,
        batched        = True,
        num_proc       = 4,
        remove_columns = dataset.column_names,
        desc           = "Train: Tokenizing and chunking..."
    )

    tokenized_data_test = data_test.map(
        tokenize_function,
        batched        = True,
        num_proc       = 4,
        remove_columns = data_test.column_names,
        desc           = "Test: Tokenizing and chunking..."
    )
    
    # 4. é…ç½®è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir                  = OUTPUT_DIR,
        overwrite_output_dir        = True,
        num_train_epochs            = NUM_EPOCHS,
        per_device_train_batch_size = BATCH_SIZE,
        per_device_eval_batch_size  = BATCH_SIZE,
        gradient_accumulation_steps = GRAD_ACCUM_STEPS,
        learning_rate               = LEARNING_RATE,
        weight_decay                = 0.01,
        logging_dir                 = f"{OUTPUT_DIR}/logs",
        logging_strategy            = "steps",
        logging_steps               = 10,
        save_strategy               = "epoch",
        # eval_strategy             = "no",
        eval_strategy               = "epoch",             # æ¯è½®åä½¿ç”¨æµ‹è¯•é›†è¿›è¡Œè¯„ä¼°
        fp16                        = FP16,
        bf16                        = False,
        optim                       = "paged_adamw_8bit",  # é…åˆ 4-bit é‡åŒ–
        lr_scheduler_type           = "cosine",
        warmup_ratio                = 0.03,
        report_to                   = "tensorboard",
        load_best_model_at_end      = False,
        ddp_find_unused_parameters  = False,
        group_by_length             = True,                # åŠ¨æ€å¡«å……
        length_column_name          = "length"             # ç”¨äº group_by_length
    )
    
    # 5. é…ç½® DataCollatorï¼ˆå…³é”®ï¼šè‡ªåŠ¨æ©ç é assistant éƒ¨åˆ†ï¼‰
    """
        mlm = true, æ©ç è¯­è¨€æ¨¡å‹ï¼š
        ä¾‹å¦‚ï¼šè¾“å…¥ -> â€œæˆ‘æ˜¨å¤©å»äº†[MASK]åº—ï¼Œä¹°äº†ä¸€ä¸ªæ±‰å ¡ã€‚â€
        æ¨¡å‹ç›®æ ‡ -> é¢„æµ‹ [MASK] ä½ç½®æ˜¯ â€œå¿«é¤â€ã€‚
    """
    data_collator = DataCollatorForLanguageModeling(
        tokenizer          = tokenizer,
        mlm                = False,  # æ˜¯å¦ä½¿ç”¨masked language modelï¼ˆæ©ç è¯­è¨€æ¨¡å‹ï¼‰ï¼Œ Falseï¼ˆå› æœè¯­è¨€æ¨¡å‹ï¼‰
        pad_to_multiple_of = 8       # ä¼˜åŒ– GPU è®¡ç®—
    )
    
    # 6. åˆ›å»º Trainer
    trainer = Trainer(
        model =  model,
        args  = training_args,
        train_dataset = tokenized_dataset,
        eval_dataset  = tokenized_data_test,
        data_collator = data_collator
    )
    
    # 7. å¼€å§‹è®­ç»ƒ
    logger.info("âœ¨ å¼€å§‹å¾®è°ƒè®­ç»ƒ...")
    train_result = trainer.train()
    
    # 8. ä¿å­˜æœ€ç»ˆæ¨¡å‹
    logger.info(f"âœ… è®­ç»ƒå®Œæˆ! ä¿å­˜æ¨¡å‹åˆ° {OUTPUT_DIR}")
    trainer.save_model(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)
    
    # 9. ä¿å­˜è®­ç»ƒæŒ‡æ ‡
    metrics = train_result.metrics
    trainer.log_metrics("train", metrics)
    trainer.save_metrics("train", metrics)
    
    # 10. éªŒè¯æ¨¡å‹ï¼ˆç®€å•æµ‹è¯•ï¼‰
    logger.info("\nğŸ” è¿›è¡Œæ¨¡å‹éªŒè¯æµ‹è¯•...")
    test_prompt = """<|im_start|>system
                    ä½ æ˜¯ä¸€ä¸ªåä¸ºæ²é›ªçš„AIå¥³å­©å­<|im_end|>
                    <|im_start|>user
                    æ²é›ªçš„åŠŸèƒ½æ˜¯ä»€ä¹ˆï¼Ÿ<|im_end|>
                    <|im_start|>assistant
                    """
    
    inputs = tokenizer(test_prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(
        **inputs,
        max_new_tokens = 100,
        do_sample      = True,
        temperature    = 0.7,
        top_p          = 0.9,
        pad_token_id   = tokenizer.eos_token_id
    )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=False)
    logger.info(f"æµ‹è¯•è¾“å…¥:\n{test_prompt}")
    logger.info(f"æ¨¡å‹è¾“å‡º:\n{response[len(test_prompt):]}")

if __name__ == "__main__":
    # æ£€æŸ¥ CUDA
    if torch.cuda.is_available():
        logger.info(f"âœ… CUDA å¯ç”¨! è®¾å¤‡: {torch.cuda.get_device_name(0)}")
        logger.info(f"æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
    else:
        logger.warning("âŒ CUDA ä¸å¯ç”¨! å°†ä½¿ç”¨ CPUï¼ˆä¸æ¨èï¼‰")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # è¿è¡Œä¸»å‡½æ•°
    main()